{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from datetime import datetime as dt\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from os import path\n",
    "spark = SparkSession.builder.appName('Test').getOrCreate()\n",
    "from pyspark.sql.functions import lit ,col\n",
    "from pyspark.sql.functions import current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+\n",
      "|hcoid|name|type|\n",
      "+-----+----+----+\n",
      "|    1|   a|  qw|\n",
      "|    2|   b|  er|\n",
      "|    3|   c|  ty|\n",
      "|    4|   d|  ui|\n",
      "|    5|   e|  op|\n",
      "|    6|   f|  as|\n",
      "|    7|   g|  df|\n",
      "|    8|   h|  gh|\n",
      "|    9|   i|  jk|\n",
      "|   10|   j|  zx|\n",
      "+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('refined/data1.csv', header = True)\n",
    "df.show()\n",
    "# df2 = df.withColumn(\"update_date\", lit(str(dt.now()))).withColumn(\"active_flag\", lit(True)  )\n",
    "\n",
    "# #df2.write.format(\"csv\").save('hco/final')\n",
    "# df2.show()\n",
    "\n",
    "#def uspertfunc(df):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing DF\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|hcoid|name|type|         update_date|active_flag|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|    1|   a|  qw|2022-03-05 17:21:...|          Y|\n",
      "|    2|   b|  er|2022-03-05 17:21:...|          Y|\n",
      "|    3|   c|  ty|2022-03-05 17:21:...|          Y|\n",
      "|    4|   d|  ui|2022-03-05 17:21:...|          Y|\n",
      "|    5|   e|  op|2022-03-05 17:21:...|          Y|\n",
      "|    6|   f|  as|2022-03-05 17:21:...|          Y|\n",
      "|    7|   g|  df|2022-03-05 17:21:...|          Y|\n",
      "|    8|   h|  gh|2022-03-05 17:21:...|          Y|\n",
      "|    9|   i|  jk|2022-03-05 17:21:...|          Y|\n",
      "|   10|   j|  zx|2022-03-05 17:21:...|          Y|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "\n",
      "delta DF\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|hcoid|name|type|         update_date|active_flag|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|    2|   b|  ab|2022-03-05 18:41:...|          Y|\n",
      "|    3|   c|  ty|2022-03-05 18:41:...|          Y|\n",
      "|    7|   g|  df|2022-03-05 18:41:...|          Y|\n",
      "|   10|   j|  zx|2022-03-05 18:41:...|          Y|\n",
      "|   11|   k|  zq|2022-03-05 18:41:...|          Y|\n",
      "|   12|   l|  xw|2022-03-05 18:41:...|          Y|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "\n",
      "Delta IDs ['2', '3', '7', '10']\n",
      "Existing DF minus updates\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|hcoid|name|type|         update_date|active_flag|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|    1|   a|  qw|2022-03-05 17:21:...|          Y|\n",
      "|    4|   d|  ui|2022-03-05 17:21:...|          Y|\n",
      "|    5|   e|  op|2022-03-05 17:21:...|          Y|\n",
      "|    6|   f|  as|2022-03-05 17:21:...|          Y|\n",
      "|    8|   h|  gh|2022-03-05 17:21:...|          Y|\n",
      "|    9|   i|  jk|2022-03-05 17:21:...|          Y|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "\n",
      "Existing DF only updates with active flag N\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|hcoid|name|type|         update_date|active_flag|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|    2|   b|  er|2022-03-05 18:41:...|          N|\n",
      "|    3|   c|  ty|2022-03-05 18:41:...|          N|\n",
      "|    7|   g|  df|2022-03-05 18:41:...|          N|\n",
      "|   10|   j|  zx|2022-03-05 18:41:...|          N|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "\n",
      "Final Data\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|hcoid|name|type|         update_date|active_flag|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|    2|   b|  ab|2022-03-05 18:41:...|          Y|\n",
      "|    3|   c|  ty|2022-03-05 18:41:...|          Y|\n",
      "|    7|   g|  df|2022-03-05 18:41:...|          Y|\n",
      "|   10|   j|  zx|2022-03-05 18:41:...|          Y|\n",
      "|   11|   k|  zq|2022-03-05 18:41:...|          Y|\n",
      "|   12|   l|  xw|2022-03-05 18:41:...|          Y|\n",
      "|    1|   a|  qw|2022-03-05 17:21:...|          Y|\n",
      "|    4|   d|  ui|2022-03-05 17:21:...|          Y|\n",
      "|    5|   e|  op|2022-03-05 17:21:...|          Y|\n",
      "|    6|   f|  as|2022-03-05 17:21:...|          Y|\n",
      "|    8|   h|  gh|2022-03-05 17:21:...|          Y|\n",
      "|    9|   i|  jk|2022-03-05 17:21:...|          Y|\n",
      "|    2|   b|  er|2022-03-05 18:41:...|          N|\n",
      "|    3|   c|  ty|2022-03-05 18:41:...|          N|\n",
      "|    7|   g|  df|2022-03-05 18:41:...|          N|\n",
      "|   10|   j|  zx|2022-03-05 18:41:...|          N|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wr_path = 'hco/final'\n",
    "col_names = ['hcoid','name','type', 'update_date','active_flag']\n",
    "\n",
    "if path.exists(\"hco/final\"):\n",
    "    df=spark.read.csv(wr_path, header = True)\n",
    "    print('existing DF')\n",
    "    df.show()\n",
    "    ddf=spark.read.csv('refined/data2.csv', header = True)\n",
    "    ddf = ddf.withColumn(\"update_date\", lit(str(dt.now()))).withColumn(\"active_flag\", lit('Y')  )\n",
    "    ddf_f = ddf\n",
    "    print('delta DF')\n",
    "    ddf_f.show()\n",
    "    df = df.withColumn(\"hcoid_m\",df[\"hcoid\"])\n",
    "    ddf = ddf.withColumn(\"hcoid_d\",ddf[\"hcoid\"])\n",
    "    #Joining Existing DF with Delta DF\n",
    "    joineddf= df.join(ddf,df.hcoid_m ==  ddf.hcoid_d,\"inner\")\n",
    "    delta_ids = joineddf.select(\"hcoid_m\").collect()\n",
    "    #convert student Name to list using\n",
    "    # flatMap\n",
    "    lst = joineddf.select('hcoid_m').rdd.flatMap(lambda x: x).collect()\n",
    "    print('Delta IDs',lst)\n",
    "    print(\"Existing DF minus updates\")\n",
    "    df_sub_delta = df.where(~col(\"hcoid\").isin(lst))\n",
    "    df_sub_delta = df_sub_delta.select(col_names)\n",
    "    df_sub_delta.show()\n",
    "    print(\"Existing DF only updates with active flag N\")\n",
    "    df_only_delta = df.where(col(\"hcoid\").isin(lst))\n",
    "    df_only_delta = df_only_delta.select(col_names[:3])\n",
    "    df_only_delta = df_only_delta.withColumn(\"update_date\", lit(str(dt.now()))).withColumn(\"active_flag\", lit('N'))\n",
    "    df_only_delta.show()\n",
    "    final = ddf_f.union(df_sub_delta).union(df_only_delta)\n",
    "    print('Final Data')\n",
    "    final.show()\n",
    "    final.repartition(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\",\"true\").save('hco_temp/final')\n",
    "    \n",
    "else:\n",
    "    print('No files in the path',e)\n",
    "    df2=spark.read.csv('refined/data1.csv', header = True)\n",
    "    df2 = df2.withColumn(\"update_date\", lit(str(dt.now()))).withColumn(\"active_flag\", lit('Y')  )\n",
    "    df2.show()\n",
    "    df2.write.format(\"csv\").mode(\"overwrite\").option(\"header\",\"true\").save('hco/final')  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-28 01:10:01.395145\n"
     ]
    }
   ],
   "source": [
    "print(dt.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+\n",
      "|hcoid|name|type|         update_date|\n",
      "+-----+----+----+--------------------+\n",
      "|    2|   b|  ab|2022-02-28 01:31:...|\n",
      "|    3|   c|  ty|2022-02-28 01:31:...|\n",
      "|    7|   g|  df|2022-02-28 01:31:...|\n",
      "|   10|   j|  zx|2022-02-28 01:31:...|\n",
      "|   11|   k|  zq|2022-02-28 01:31:...|\n",
      "|   12|   l|  xw|2022-02-28 01:31:...|\n",
      "+-----+----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ddf=spark.read.csv('refined/data2.csv', header = True)\n",
    "ddf = ddf.withColumn(\"update_date\", lit(str(dt.now())))\n",
    "ddf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+--------------------+-----------+\n",
      "|hcoid|name|type|         update_date|active_flag|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "|    1|   a|  qw|2022-02-28 01:32:...|          Y|\n",
      "|    2|   b|  er|2022-02-28 01:32:...|          Y|\n",
      "|    3|   c|  ty|2022-02-28 01:32:...|          Y|\n",
      "|    4|   d|  ui|2022-02-28 01:32:...|          Y|\n",
      "|    5|   e|  op|2022-02-28 01:32:...|          Y|\n",
      "|    6|   f|  as|2022-02-28 01:32:...|          Y|\n",
      "|    7|   g|  df|2022-02-28 01:32:...|          Y|\n",
      "|    8|   h|  gh|2022-02-28 01:32:...|          Y|\n",
      "|    9|   i|  jk|2022-02-28 01:32:...|          Y|\n",
      "|   10|   j|  zx|2022-02-28 01:32:...|          Y|\n",
      "+-----+----+----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.read.csv('refined/data1.csv', header = True)\n",
    "df2 = df2.withColumn(\"update_date\", lit(str(dt.now()))).withColumn(\"active_flag\",  lit(str('Y')) )\n",
    "df2.show()\n",
    "#df.show()\n",
    "#df2.write.format(\"csv\").mode(\"overwrite\").option(\"header\",\"true\").save('hco/final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
